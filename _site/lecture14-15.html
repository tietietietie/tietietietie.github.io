<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>FaRM and Spark</title>

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="FaRM and Spark" />
<meta name="author" content="tie" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Lecture 14: FaRM Video 1,和spanner类似，采用replication + 2pc commit + sharding，不同点在于，spanner用于实际场景的全国范围的分布式数据库，而FaRM的副本存储在同一个DC，为了使用高性能的RDMA NIC而设计。速度上，spanner一次读写事务需要10ms，而FaRM通常只需要50多微妙。瓶颈上，Spanner的性能瓶颈在于网络延迟，而FaRM在于CPU速度。 2，FaRM设置：使用zookeeper管理集群，使用主从 replication，F+1的服务器能容忍F台宕机。 3，性能提高原因：1，分片提高并行能力。2，数据存储在非易失性RAM，读写很快。3，使用RDMA，不用造成系统中断。Kernal ByPass，应用程序直接读取网络上的数据，不用经过内核。 4，NVRAM：没有persist()，节省时间，RAM比SSD快几个数量级。如何实现非易失性？有一个备用电源，当检测到主电源断电，所有RAM立刻把数据写到与之关联的SSD中，并自动结束运行。 5，介绍RPC的包怎么在网络中传输：比较慢，涉及多层，以及数据复制，系统中断等。 6，FaRM提出了解决网络传输瓶颈的两个办法： 1，Kernal Bypass: NIC能够直接读取内存中的app data，app data也能直接传输到NIC。此时我们的app不需要经过buffer/TCP等内核的操作，得自己写这部分功能，可以借助DPDK包。（当然NIC必须支持DMA） ２，RDMA技术：能直接通过目标服务器的RDMA NIC,读取app中的数据, 提供类似TCP的功能.,也只适用于本地网络集群. RDMA会与transaction冲突,因为可能会读到未commit的数据,或者被锁起来的数据??(RDMA也不会和软件进行任何交流?就只是直接读数据?) 为了避免使用锁,可以(必须)用乐观并发控制(OCC):不用锁进行读操作,写操作缓存(缓存在transaction调用方),整个事务操作结束时,有一个验证阶段,验证是否满足 读操作非常快,用的是one-side RDMA. 7,FaRM进行一次事务的过程(API) txCreate() o = tx.Read(OID) o.x += 1 txWrite(OID, o) ok = txCommit() 两个要注意的点: 1)OID组成:region # address,其中region表明了主从服务器的位置, address表明了RAM中要读写数据的地址 2)服务器的内存结构: 如何验证和提交transaction? 如何验证满足一致性呢? 执行阶段: 1, tx的发起者C需要使用one-side RDMA中p1,p2,p3中读取数据(非常快)(任何要写/修改的数据,首先也要read,获得版本号)(发起者起到了2pc中TC的角色) Commit阶段 1,LOCK: primary会检查object的版本号和锁bit,如果版本号和从TC中传来的object版本号不一致,或者primary的object的lock bit已经上锁,则给TC返回false,否则,加锁并返回true. (注意比较版本号和Lock bit, 已经返回true/false,是一次原子性操作)(不需要等锁释放) 2,如果TC收到了全部yes, TC决定commit, TC通知全部的backup和primary. 注意当primary收到了TC可以commit的信号后,修改object的版本号,并把lock bit清空. 3,全部提交后,通知primary和backup删除此次log Validate: 用来验证transaction中的read操作, 保证validate时(refetch时),其lock和version number都没有改变. 举例1 TC1和TC2并发执行 x += 1,会出现以下情况: 1, TC1和TC2同时读x发送lock phase, 此时由于原子性操作, 只会有一个TC会返回true, x最终加1 2,TC1和TC同时读x,但是TC1先commit, TC2也不会执行,因为版本号不对. 最大特点是read操作时OCC的,非常快,不需要锁. 举例2 验证强一致性的典型例子 如果T1和T2同步进行,则它们都会修改x = 1和 y = 1,但是在validation阶段,两个return false,从而保证线性化. Lecture15: Spark Paper: Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing 1，概述 RDD的两个特定：1，容错特性。2，基于内存。 解决问题：循环数据流 RDD实现容错很简单：因为RDD是高度受限的只读内存模型，只能通过与其他RDD进行转换操作（map, join, group by）创建。不需要check-point和roll-back机制，它们使用Lineage重建丢失的分区。 利用RDD内在的确定性特性，创建了一种Spark调试工具rddbg，允许用户在任务期间利用Lineage重建RDD，然后像传统调试器那样重新执行任务 2，RDD概念 设计目标：为基于工作集的应用（多个并行操作重用中间结果的应用）提供抽象，并保留MapReduce的特性（自动容错/位置感知/可伸缩）。 RDD的容错方式：通过记录数据更新实现容错，为防止更新过多，RDD只支持粗粒度转换。粗粒度支持数据并行的批量分析应用,不适用于异步更新共享状态的应用. RDD不需要物化,RDD含有如何从其他RDD衍生（即计算）出本RDD的相关信息（即Lineage），据此可以从物理存储的数据计算出相应的RDD分区。 RDD编程模型： 1,RDD在ｓｐａｒｋ中被表示为对象.通过对象上的方法进行调用和转换. 2,RDD通过动作，向应用程序返回值.如count/collect/save(将RDD的结果输出到存储系统),也就是说只有在动作时,才会计算RDD（延时计算） ３，可以通过缓存和分区控制RDD,缓存可以将计算好的RDD存在缓存上，加速后期重用．分区有哈希分区和范围分区两种,应用程序可以将两个RDD进行哈希分区，加速ｊｏｉｎ操作． 我所理解的RDD模型: 一种包含了对本地数据进行Lineage操作的抽象数据模型,只能通过批量操作创建(不能单独写),使用粗粒度更新,发生故障可以利用其他的RDD恢复. 与共享式内存模型对比(DSM) Video 分布式计算,相比于MapReduce, 对循环迭代有更好的支持,提供数据流?(dataflow). 能够将之前需要多个MapReduce的过程组合在一起。 PageRank 1 val lines = spark.read.textFile(&quot;in&quot;).rdd //得到一个RDD对象？（类似于线性图/流程图？）不进行任何的计算 //只有在lines.collect()时，才进行计算，如果在此处collect()，会返回in里面的数据（string形式） 2 val links1 = lines.map{ s =&gt; 3 val parts = s.split(&quot;\\s+&quot;) 4 (parts(0), parts(1)) 5 } //这里得到一个RDD对象，将lines转换成了map，但是也不会真正进行操作，只是线性图？ 6 val links2 = links1.distinct() //需要分区之间的网络交流,消除重复项 7 val links3 = links2.groupByKey() //根据from URL,将link分组(可能不需要网络交流,上一步可能已经sort并把相同的key放在同一分区) //接下来迭代将会反复用到link3 8 val links4 = links3.cache() //之前的collect,会反复读取in文件,所以需要进行cache 9 var ranks = links4.mapValues(v =&gt; 1.0) 10 //把所有页面的概率初始化为1 11 for (i &lt;- 1 to 10) { 12 val jj = links4.join(ranks) //大范围的重新组织数据,把ranks和links4的相同key item组合在一起,形成新的对象(如果spark把links4和ranks的相同 //key放在一个分区就好了) 13 val contribs = jj.values.flatMap{ 14 case (urls, rank) =&gt; 15 urls.map(url =&gt; (url, rank / urls.size)) 16 } 17 ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _) //通过某种算法,计算出来新的rank 18 } 19 20 val output = ranks.collect() 21 output.foreach(tup =&gt; println(s&quot;${tup._1} has rank: ${tup._2} .&quot;)) 用于计算web网页的重要性。也可以表示用户访问此网站的概率。此算法不断模拟（迭代）用户的随机点击行为。最终收集的结果，会逼近网站的真正点击概率。 如果使用MapReduce，将会很麻烦和缓慢，因为一次模拟行为就需要一个mapreduce来模拟。（且不断的从GFS磁盘读写） 注意以上代码:我们并没有指定分区位置, 如何分区等操作,这些都是spark内部完成. 对象jj的值如下: 如何在分布式系统上执行上述计算 1, worker在HDFS上读取数据(很可能该worker就储存了多块HDFS分片) 2,对读取的数据进行map (spark支持stream操作,也就是说读数据的时候同时Map) 3,distinct需要worker之间的通信,具体做法是,每个worker会按照fromURL(key),把数据分成多个小份存储, worker只需要从所有服务器中,把属于自己的小份取走,形成新的分区. 1,2为Narrow transformation, 服务器可以本地的对数据进行操作,不需要互相通信. 3为wide transfromation,需要服务器之间的数据传递,非常耗时. 4, spark的一个优势是,在执行操作前,已经知道所有操作的lineage graph,从而有优化的空间. 所以在gruopByKey(),可以不需要在重新分区了. 容错 容错是指对重新全部计算进行容错. spark的某个worker失效后,重头开始计算即可. 但存在一个问题,即该worker执行到需要wide transformation的操作时,可能需要来自其他worker的数据, spark是在内存中计算的, 此操作的结果可能被丢弃了, 为了避免其余所有的worker重新计算来产生丢弃的数据, 利用显式的persist()操作,把数据存起来. 比如在pagerank中，可以每10次迭代，把结果保存到HDFS。（可以用cache吗？不确定，因为cache可以只是建议性的把数据暂时保存在内存？）" />
<meta property="og:description" content="Lecture 14: FaRM Video 1,和spanner类似，采用replication + 2pc commit + sharding，不同点在于，spanner用于实际场景的全国范围的分布式数据库，而FaRM的副本存储在同一个DC，为了使用高性能的RDMA NIC而设计。速度上，spanner一次读写事务需要10ms，而FaRM通常只需要50多微妙。瓶颈上，Spanner的性能瓶颈在于网络延迟，而FaRM在于CPU速度。 2，FaRM设置：使用zookeeper管理集群，使用主从 replication，F+1的服务器能容忍F台宕机。 3，性能提高原因：1，分片提高并行能力。2，数据存储在非易失性RAM，读写很快。3，使用RDMA，不用造成系统中断。Kernal ByPass，应用程序直接读取网络上的数据，不用经过内核。 4，NVRAM：没有persist()，节省时间，RAM比SSD快几个数量级。如何实现非易失性？有一个备用电源，当检测到主电源断电，所有RAM立刻把数据写到与之关联的SSD中，并自动结束运行。 5，介绍RPC的包怎么在网络中传输：比较慢，涉及多层，以及数据复制，系统中断等。 6，FaRM提出了解决网络传输瓶颈的两个办法： 1，Kernal Bypass: NIC能够直接读取内存中的app data，app data也能直接传输到NIC。此时我们的app不需要经过buffer/TCP等内核的操作，得自己写这部分功能，可以借助DPDK包。（当然NIC必须支持DMA） ２，RDMA技术：能直接通过目标服务器的RDMA NIC,读取app中的数据, 提供类似TCP的功能.,也只适用于本地网络集群. RDMA会与transaction冲突,因为可能会读到未commit的数据,或者被锁起来的数据??(RDMA也不会和软件进行任何交流?就只是直接读数据?) 为了避免使用锁,可以(必须)用乐观并发控制(OCC):不用锁进行读操作,写操作缓存(缓存在transaction调用方),整个事务操作结束时,有一个验证阶段,验证是否满足 读操作非常快,用的是one-side RDMA. 7,FaRM进行一次事务的过程(API) txCreate() o = tx.Read(OID) o.x += 1 txWrite(OID, o) ok = txCommit() 两个要注意的点: 1)OID组成:region # address,其中region表明了主从服务器的位置, address表明了RAM中要读写数据的地址 2)服务器的内存结构: 如何验证和提交transaction? 如何验证满足一致性呢? 执行阶段: 1, tx的发起者C需要使用one-side RDMA中p1,p2,p3中读取数据(非常快)(任何要写/修改的数据,首先也要read,获得版本号)(发起者起到了2pc中TC的角色) Commit阶段 1,LOCK: primary会检查object的版本号和锁bit,如果版本号和从TC中传来的object版本号不一致,或者primary的object的lock bit已经上锁,则给TC返回false,否则,加锁并返回true. (注意比较版本号和Lock bit, 已经返回true/false,是一次原子性操作)(不需要等锁释放) 2,如果TC收到了全部yes, TC决定commit, TC通知全部的backup和primary. 注意当primary收到了TC可以commit的信号后,修改object的版本号,并把lock bit清空. 3,全部提交后,通知primary和backup删除此次log Validate: 用来验证transaction中的read操作, 保证validate时(refetch时),其lock和version number都没有改变. 举例1 TC1和TC2并发执行 x += 1,会出现以下情况: 1, TC1和TC2同时读x发送lock phase, 此时由于原子性操作, 只会有一个TC会返回true, x最终加1 2,TC1和TC同时读x,但是TC1先commit, TC2也不会执行,因为版本号不对. 最大特点是read操作时OCC的,非常快,不需要锁. 举例2 验证强一致性的典型例子 如果T1和T2同步进行,则它们都会修改x = 1和 y = 1,但是在validation阶段,两个return false,从而保证线性化. Lecture15: Spark Paper: Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing 1，概述 RDD的两个特定：1，容错特性。2，基于内存。 解决问题：循环数据流 RDD实现容错很简单：因为RDD是高度受限的只读内存模型，只能通过与其他RDD进行转换操作（map, join, group by）创建。不需要check-point和roll-back机制，它们使用Lineage重建丢失的分区。 利用RDD内在的确定性特性，创建了一种Spark调试工具rddbg，允许用户在任务期间利用Lineage重建RDD，然后像传统调试器那样重新执行任务 2，RDD概念 设计目标：为基于工作集的应用（多个并行操作重用中间结果的应用）提供抽象，并保留MapReduce的特性（自动容错/位置感知/可伸缩）。 RDD的容错方式：通过记录数据更新实现容错，为防止更新过多，RDD只支持粗粒度转换。粗粒度支持数据并行的批量分析应用,不适用于异步更新共享状态的应用. RDD不需要物化,RDD含有如何从其他RDD衍生（即计算）出本RDD的相关信息（即Lineage），据此可以从物理存储的数据计算出相应的RDD分区。 RDD编程模型： 1,RDD在ｓｐａｒｋ中被表示为对象.通过对象上的方法进行调用和转换. 2,RDD通过动作，向应用程序返回值.如count/collect/save(将RDD的结果输出到存储系统),也就是说只有在动作时,才会计算RDD（延时计算） ３，可以通过缓存和分区控制RDD,缓存可以将计算好的RDD存在缓存上，加速后期重用．分区有哈希分区和范围分区两种,应用程序可以将两个RDD进行哈希分区，加速ｊｏｉｎ操作． 我所理解的RDD模型: 一种包含了对本地数据进行Lineage操作的抽象数据模型,只能通过批量操作创建(不能单独写),使用粗粒度更新,发生故障可以利用其他的RDD恢复. 与共享式内存模型对比(DSM) Video 分布式计算,相比于MapReduce, 对循环迭代有更好的支持,提供数据流?(dataflow). 能够将之前需要多个MapReduce的过程组合在一起。 PageRank 1 val lines = spark.read.textFile(&quot;in&quot;).rdd //得到一个RDD对象？（类似于线性图/流程图？）不进行任何的计算 //只有在lines.collect()时，才进行计算，如果在此处collect()，会返回in里面的数据（string形式） 2 val links1 = lines.map{ s =&gt; 3 val parts = s.split(&quot;\\s+&quot;) 4 (parts(0), parts(1)) 5 } //这里得到一个RDD对象，将lines转换成了map，但是也不会真正进行操作，只是线性图？ 6 val links2 = links1.distinct() //需要分区之间的网络交流,消除重复项 7 val links3 = links2.groupByKey() //根据from URL,将link分组(可能不需要网络交流,上一步可能已经sort并把相同的key放在同一分区) //接下来迭代将会反复用到link3 8 val links4 = links3.cache() //之前的collect,会反复读取in文件,所以需要进行cache 9 var ranks = links4.mapValues(v =&gt; 1.0) 10 //把所有页面的概率初始化为1 11 for (i &lt;- 1 to 10) { 12 val jj = links4.join(ranks) //大范围的重新组织数据,把ranks和links4的相同key item组合在一起,形成新的对象(如果spark把links4和ranks的相同 //key放在一个分区就好了) 13 val contribs = jj.values.flatMap{ 14 case (urls, rank) =&gt; 15 urls.map(url =&gt; (url, rank / urls.size)) 16 } 17 ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _) //通过某种算法,计算出来新的rank 18 } 19 20 val output = ranks.collect() 21 output.foreach(tup =&gt; println(s&quot;${tup._1} has rank: ${tup._2} .&quot;)) 用于计算web网页的重要性。也可以表示用户访问此网站的概率。此算法不断模拟（迭代）用户的随机点击行为。最终收集的结果，会逼近网站的真正点击概率。 如果使用MapReduce，将会很麻烦和缓慢，因为一次模拟行为就需要一个mapreduce来模拟。（且不断的从GFS磁盘读写） 注意以上代码:我们并没有指定分区位置, 如何分区等操作,这些都是spark内部完成. 对象jj的值如下: 如何在分布式系统上执行上述计算 1, worker在HDFS上读取数据(很可能该worker就储存了多块HDFS分片) 2,对读取的数据进行map (spark支持stream操作,也就是说读数据的时候同时Map) 3,distinct需要worker之间的通信,具体做法是,每个worker会按照fromURL(key),把数据分成多个小份存储, worker只需要从所有服务器中,把属于自己的小份取走,形成新的分区. 1,2为Narrow transformation, 服务器可以本地的对数据进行操作,不需要互相通信. 3为wide transfromation,需要服务器之间的数据传递,非常耗时. 4, spark的一个优势是,在执行操作前,已经知道所有操作的lineage graph,从而有优化的空间. 所以在gruopByKey(),可以不需要在重新分区了. 容错 容错是指对重新全部计算进行容错. spark的某个worker失效后,重头开始计算即可. 但存在一个问题,即该worker执行到需要wide transformation的操作时,可能需要来自其他worker的数据, spark是在内存中计算的, 此操作的结果可能被丢弃了, 为了避免其余所有的worker重新计算来产生丢弃的数据, 利用显式的persist()操作,把数据存起来. 比如在pagerank中，可以每10次迭代，把结果保存到HDFS。（可以用cache吗？不确定，因为cache可以只是建议性的把数据暂时保存在内存？）" />
<link rel="canonical" href="https://github.com/tietietietie/tietietietie.github.io/lecture14-15.html" />
<meta property="og:url" content="https://github.com/tietietietie/tietietietie.github.io/lecture14-15.html" />
<meta property="og:site_name" content="tie’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-05T06:58:00+08:00" />
<script type="application/ld+json">
{"url":"https://github.com/tietietietie/tietietietie.github.io/lecture14-15.html","headline":"FaRM and Spark","dateModified":"2020-11-05T06:58:00+08:00","datePublished":"2020-11-05T06:58:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/tietietietie/tietietietie.github.io/lecture14-15.html"},"author":{"@type":"Person","name":"tie"},"description":"Lecture 14: FaRM Video 1,和spanner类似，采用replication + 2pc commit + sharding，不同点在于，spanner用于实际场景的全国范围的分布式数据库，而FaRM的副本存储在同一个DC，为了使用高性能的RDMA NIC而设计。速度上，spanner一次读写事务需要10ms，而FaRM通常只需要50多微妙。瓶颈上，Spanner的性能瓶颈在于网络延迟，而FaRM在于CPU速度。 2，FaRM设置：使用zookeeper管理集群，使用主从 replication，F+1的服务器能容忍F台宕机。 3，性能提高原因：1，分片提高并行能力。2，数据存储在非易失性RAM，读写很快。3，使用RDMA，不用造成系统中断。Kernal ByPass，应用程序直接读取网络上的数据，不用经过内核。 4，NVRAM：没有persist()，节省时间，RAM比SSD快几个数量级。如何实现非易失性？有一个备用电源，当检测到主电源断电，所有RAM立刻把数据写到与之关联的SSD中，并自动结束运行。 5，介绍RPC的包怎么在网络中传输：比较慢，涉及多层，以及数据复制，系统中断等。 6，FaRM提出了解决网络传输瓶颈的两个办法： 1，Kernal Bypass: NIC能够直接读取内存中的app data，app data也能直接传输到NIC。此时我们的app不需要经过buffer/TCP等内核的操作，得自己写这部分功能，可以借助DPDK包。（当然NIC必须支持DMA） ２，RDMA技术：能直接通过目标服务器的RDMA NIC,读取app中的数据, 提供类似TCP的功能.,也只适用于本地网络集群. RDMA会与transaction冲突,因为可能会读到未commit的数据,或者被锁起来的数据??(RDMA也不会和软件进行任何交流?就只是直接读数据?) 为了避免使用锁,可以(必须)用乐观并发控制(OCC):不用锁进行读操作,写操作缓存(缓存在transaction调用方),整个事务操作结束时,有一个验证阶段,验证是否满足 读操作非常快,用的是one-side RDMA. 7,FaRM进行一次事务的过程(API) txCreate() o = tx.Read(OID) o.x += 1 txWrite(OID, o) ok = txCommit() 两个要注意的点: 1)OID组成:region # address,其中region表明了主从服务器的位置, address表明了RAM中要读写数据的地址 2)服务器的内存结构: 如何验证和提交transaction? 如何验证满足一致性呢? 执行阶段: 1, tx的发起者C需要使用one-side RDMA中p1,p2,p3中读取数据(非常快)(任何要写/修改的数据,首先也要read,获得版本号)(发起者起到了2pc中TC的角色) Commit阶段 1,LOCK: primary会检查object的版本号和锁bit,如果版本号和从TC中传来的object版本号不一致,或者primary的object的lock bit已经上锁,则给TC返回false,否则,加锁并返回true. (注意比较版本号和Lock bit, 已经返回true/false,是一次原子性操作)(不需要等锁释放) 2,如果TC收到了全部yes, TC决定commit, TC通知全部的backup和primary. 注意当primary收到了TC可以commit的信号后,修改object的版本号,并把lock bit清空. 3,全部提交后,通知primary和backup删除此次log Validate: 用来验证transaction中的read操作, 保证validate时(refetch时),其lock和version number都没有改变. 举例1 TC1和TC2并发执行 x += 1,会出现以下情况: 1, TC1和TC2同时读x发送lock phase, 此时由于原子性操作, 只会有一个TC会返回true, x最终加1 2,TC1和TC同时读x,但是TC1先commit, TC2也不会执行,因为版本号不对. 最大特点是read操作时OCC的,非常快,不需要锁. 举例2 验证强一致性的典型例子 如果T1和T2同步进行,则它们都会修改x = 1和 y = 1,但是在validation阶段,两个return false,从而保证线性化. Lecture15: Spark Paper: Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing 1，概述 RDD的两个特定：1，容错特性。2，基于内存。 解决问题：循环数据流 RDD实现容错很简单：因为RDD是高度受限的只读内存模型，只能通过与其他RDD进行转换操作（map, join, group by）创建。不需要check-point和roll-back机制，它们使用Lineage重建丢失的分区。 利用RDD内在的确定性特性，创建了一种Spark调试工具rddbg，允许用户在任务期间利用Lineage重建RDD，然后像传统调试器那样重新执行任务 2，RDD概念 设计目标：为基于工作集的应用（多个并行操作重用中间结果的应用）提供抽象，并保留MapReduce的特性（自动容错/位置感知/可伸缩）。 RDD的容错方式：通过记录数据更新实现容错，为防止更新过多，RDD只支持粗粒度转换。粗粒度支持数据并行的批量分析应用,不适用于异步更新共享状态的应用. RDD不需要物化,RDD含有如何从其他RDD衍生（即计算）出本RDD的相关信息（即Lineage），据此可以从物理存储的数据计算出相应的RDD分区。 RDD编程模型： 1,RDD在ｓｐａｒｋ中被表示为对象.通过对象上的方法进行调用和转换. 2,RDD通过动作，向应用程序返回值.如count/collect/save(将RDD的结果输出到存储系统),也就是说只有在动作时,才会计算RDD（延时计算） ３，可以通过缓存和分区控制RDD,缓存可以将计算好的RDD存在缓存上，加速后期重用．分区有哈希分区和范围分区两种,应用程序可以将两个RDD进行哈希分区，加速ｊｏｉｎ操作． 我所理解的RDD模型: 一种包含了对本地数据进行Lineage操作的抽象数据模型,只能通过批量操作创建(不能单独写),使用粗粒度更新,发生故障可以利用其他的RDD恢复. 与共享式内存模型对比(DSM) Video 分布式计算,相比于MapReduce, 对循环迭代有更好的支持,提供数据流?(dataflow). 能够将之前需要多个MapReduce的过程组合在一起。 PageRank 1 val lines = spark.read.textFile(&quot;in&quot;).rdd //得到一个RDD对象？（类似于线性图/流程图？）不进行任何的计算 //只有在lines.collect()时，才进行计算，如果在此处collect()，会返回in里面的数据（string形式） 2 val links1 = lines.map{ s =&gt; 3 val parts = s.split(&quot;\\\\s+&quot;) 4 (parts(0), parts(1)) 5 } //这里得到一个RDD对象，将lines转换成了map，但是也不会真正进行操作，只是线性图？ 6 val links2 = links1.distinct() //需要分区之间的网络交流,消除重复项 7 val links3 = links2.groupByKey() //根据from URL,将link分组(可能不需要网络交流,上一步可能已经sort并把相同的key放在同一分区) //接下来迭代将会反复用到link3 8 val links4 = links3.cache() //之前的collect,会反复读取in文件,所以需要进行cache 9 var ranks = links4.mapValues(v =&gt; 1.0) 10 //把所有页面的概率初始化为1 11 for (i &lt;- 1 to 10) { 12 val jj = links4.join(ranks) //大范围的重新组织数据,把ranks和links4的相同key item组合在一起,形成新的对象(如果spark把links4和ranks的相同 //key放在一个分区就好了) 13 val contribs = jj.values.flatMap{ 14 case (urls, rank) =&gt; 15 urls.map(url =&gt; (url, rank / urls.size)) 16 } 17 ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _) //通过某种算法,计算出来新的rank 18 } 19 20 val output = ranks.collect() 21 output.foreach(tup =&gt; println(s&quot;${tup._1} has rank: ${tup._2} .&quot;)) 用于计算web网页的重要性。也可以表示用户访问此网站的概率。此算法不断模拟（迭代）用户的随机点击行为。最终收集的结果，会逼近网站的真正点击概率。 如果使用MapReduce，将会很麻烦和缓慢，因为一次模拟行为就需要一个mapreduce来模拟。（且不断的从GFS磁盘读写） 注意以上代码:我们并没有指定分区位置, 如何分区等操作,这些都是spark内部完成. 对象jj的值如下: 如何在分布式系统上执行上述计算 1, worker在HDFS上读取数据(很可能该worker就储存了多块HDFS分片) 2,对读取的数据进行map (spark支持stream操作,也就是说读数据的时候同时Map) 3,distinct需要worker之间的通信,具体做法是,每个worker会按照fromURL(key),把数据分成多个小份存储, worker只需要从所有服务器中,把属于自己的小份取走,形成新的分区. 1,2为Narrow transformation, 服务器可以本地的对数据进行操作,不需要互相通信. 3为wide transfromation,需要服务器之间的数据传递,非常耗时. 4, spark的一个优势是,在执行操作前,已经知道所有操作的lineage graph,从而有优化的空间. 所以在gruopByKey(),可以不需要在重新分区了. 容错 容错是指对重新全部计算进行容错. spark的某个worker失效后,重头开始计算即可. 但存在一个问题,即该worker执行到需要wide transformation的操作时,可能需要来自其他worker的数据, spark是在内存中计算的, 此操作的结果可能被丢弃了, 为了避免其余所有的worker重新计算来产生丢弃的数据, 利用显式的persist()操作,把数据存起来. 比如在pagerank中，可以每10次迭代，把结果保存到HDFS。（可以用cache吗？不确定，因为cache可以只是建议性的把数据暂时保存在内存？）","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link type="application/atom+xml" rel="alternate" href="https://github.com/tietietietie/tietietietie.github.io/feed.xml" title="tie's blog" />

  <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
  <link rel="stylesheet" href="/assets/css/main.css" />
</head><body a="auto">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/">..</a>

<article>
  <p class="post-meta">
    <time datetime="2020-11-05 06:58:00 +0800">2020-11-05</time>
  </p>
  
  <h1>FaRM and Spark</h1>

  <h1 id="lecture-14-farm">Lecture 14: FaRM</h1>

<h2 id="video">Video</h2>

<p>1,和spanner类似，采用replication + 2pc commit + sharding，不同点在于，spanner用于实际场景的全国范围的分布式数据库，而FaRM的副本存储在同一个DC，为了使用高性能的RDMA NIC而设计。速度上，spanner一次读写事务需要10ms，而FaRM通常只需要50多微妙。瓶颈上，Spanner的性能瓶颈在于网络延迟，而FaRM在于CPU速度。</p>

<p>2，FaRM设置：使用zookeeper管理集群，使用主从 replication，F+1的服务器能容忍F台宕机。</p>

<p>3，性能提高原因：1，分片提高并行能力。2，数据存储在非易失性RAM，读写很快。3，使用RDMA，不用造成系统中断。Kernal ByPass，应用程序直接读取网络上的数据，不用经过内核。</p>

<p>4，NVRAM：没有persist()，节省时间，RAM比SSD快几个数量级。如何实现非易失性？有一个备用电源，当检测到主电源断电，所有RAM立刻把数据写到与之关联的SSD中，并自动结束运行。</p>

<p>5，介绍RPC的包怎么在网络中传输：比较慢，涉及多层，以及数据复制，系统中断等。</p>

<p><a href="https://imgchr.com/i/yhhkLQ"><img src="https://s3.ax1x.com/2021/02/19/yhhkLQ.png" alt="yhhkLQ.png" /></a></p>

<p>6，FaRM提出了解决网络传输瓶颈的两个办法：</p>

<p>1，Kernal Bypass: NIC能够直接读取内存中的app data，app data也能直接传输到NIC。此时我们的app不需要经过buffer/TCP等内核的操作，得自己写这部分功能，可以借助DPDK包。（当然NIC必须支持DMA）</p>

<p>２，RDMA技术：能直接通过目标服务器的RDMA NIC,读取app中的数据, 提供类似TCP的功能.,也只适用于本地网络集群.</p>

<p>RDMA会与transaction冲突,因为可能会读到未commit的数据,或者被锁起来的数据??(RDMA也不会和软件进行任何交流?就只是直接读数据?)</p>

<p><strong>为了避免使用锁,可以(必须)用乐观并发控制(OCC):</strong>不用锁进行读操作,写操作缓存(缓存在transaction调用方),整个事务操作结束时,有一个验证阶段,验证是否满足</p>

<p><strong>读操作非常快,用的是one-side RDMA.</strong></p>

<p>7,FaRM进行一次事务的过程(API)</p>

<p>txCreate()</p>

<p>o = tx.Read(OID)</p>

<p>o.x += 1</p>

<p>txWrite(OID, o)</p>

<p>ok = txCommit()</p>

<p>两个要注意的点:</p>

<p>1)OID组成:region # address,其中region表明了主从服务器的位置, address表明了RAM中要读写数据的地址</p>

<p>2)服务器的内存结构:</p>

<p><a href="https://imgchr.com/i/yhhpJP"><img src="https://s3.ax1x.com/2021/02/19/yhhpJP.png" alt="yhhpJP.png" /></a></p>

<h3 id="如何验证和提交transaction">如何验证和提交transaction?</h3>

<p>如何验证满足一致性呢?</p>

<p><a href="https://imgchr.com/i/yhh9Rf"><img src="https://s3.ax1x.com/2021/02/19/yhh9Rf.png" alt="yhh9Rf.png" /></a></p>

<p>执行阶段:</p>

<p>1, tx的发起者C需要使用one-side RDMA中p1,p2,p3中读取数据(非常快)(任何要写/修改的数据,首先也要read,获得版本号)(发起者起到了2pc中TC的角色)</p>

<p>Commit阶段</p>

<p>1,LOCK: primary会检查object的版本号和锁bit,如果版本号和从TC中传来的object版本号不一致,或者primary的object的lock bit已经上锁,则给TC返回false,否则,加锁并返回true. (注意比较版本号和Lock bit, 已经返回true/false,是一次原子性操作)(不需要等锁释放)</p>

<p>2,如果TC收到了全部yes, TC决定commit, TC通知全部的backup和primary. 注意当primary收到了TC可以commit的信号后,修改object的版本号,并把lock bit清空.</p>

<p>3,全部提交后,通知primary和backup删除此次log</p>

<p>Validate: 用来验证transaction中的read操作, 保证validate时(refetch时),其lock和version number都没有改变.</p>

<h3 id="举例1">举例1</h3>

<p>TC1和TC2并发执行 x += 1,会出现以下情况:</p>

<p>1, TC1和TC2同时读x发送lock phase, 此时由于原子性操作, 只会有一个TC会返回true, x最终加1</p>

<p>2,TC1和TC同时读x,但是TC1先commit, TC2也不会执行,因为版本号不对.</p>

<p>最大特点是read操作时OCC的,非常快,不需要锁.</p>

<h3 id="举例2">举例2</h3>

<p>验证强一致性的典型例子</p>

<p><a href="https://imgchr.com/i/yhhiQS"><img src="https://s3.ax1x.com/2021/02/19/yhhiQS.png" alt="yhhiQS.png" /></a></p>

<p>如果T1和T2同步进行,则它们都会修改x = 1和 y = 1,但是在validation阶段,两个return false,从而保证线性化.</p>

<h1 id="lecture15-spark">Lecture15: Spark</h1>

<h2 id="paper--resilient-distributed-datasets-a-fault-tolerant-abstraction-for-in-memory-cluster-computing">Paper:  Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</h2>

<h3 id="1概述">1，概述</h3>

<p>RDD的两个特定：1，容错特性。2，基于内存。</p>

<p>解决问题：循环数据流</p>

<p>RDD实现容错很简单：因为RDD是高度受限的只读内存模型，只能通过与其他RDD进行转换操作（map, join, group by）创建。不需要check-point和roll-back机制，它们使用Lineage重建丢失的分区。</p>

<p>利用RDD内在的确定性特性，创建了一种<strong>Spark调试工具rddbg</strong>，允许用户在任务期间利用Lineage重建RDD，然后像传统调试器那样重新执行任务</p>

<h3 id="2rdd概念">2，RDD概念</h3>

<p>设计目标：为基于工作集的应用（多个并行操作重用中间结果的应用）提供抽象，并保留MapReduce的特性（自动容错/位置感知/可伸缩）。</p>

<p>RDD的容错方式：通过记录数据更新实现容错，为防止更新过多，RDD只支持粗粒度转换。粗粒度支持数据并行的批量分析应用,不适用于异步更新共享状态的应用.</p>

<p>RDD不需要物化,RDD含有如何从其他RDD衍生（即计算）出本RDD的相关信息（即Lineage），据此可以从物理存储的数据计算出相应的RDD分区。</p>

<p><strong>RDD编程模型</strong>：</p>

<p>1,RDD在ｓｐａｒｋ中被表示为对象.通过对象上的方法进行调用和转换.</p>

<p>2,RDD通过动作，向应用程序返回值.如count/collect/save(将RDD的结果输出到存储系统),也就是说只有在动作时,才会计算RDD（延时计算）</p>

<p>３，可以通过缓存和分区控制RDD,缓存可以将计算好的RDD存在缓存上，加速后期重用．分区有哈希分区和范围分区两种,应用程序可以将两个RDD进行哈希分区，加速ｊｏｉｎ操作．</p>

<p>我所理解的RDD模型: 一种包含了对本地数据进行Lineage操作的抽象数据模型,只能通过批量操作创建(不能单独写),使用粗粒度更新,发生故障可以利用其他的RDD恢复.</p>

<p>与共享式内存模型对比(DSM)</p>

<p><a href="https://imgchr.com/i/yhhFsg"><img src="https://s3.ax1x.com/2021/02/19/yhhFsg.png" alt="yhhFsg.png" /></a></p>

<h2 id="video-1">Video</h2>

<p>分布式计算,相比于MapReduce, 对循环迭代有更好的支持,提供数据流?(dataflow). 能够将之前需要多个MapReduce的过程组合在一起。</p>

<h3 id="pagerank">PageRank</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	 <span class="mi">1</span>		<span class="k">val</span> <span class="nv">lines</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"in"</span><span class="o">).</span><span class="py">rdd</span>
			<span class="c1">//得到一个RDD对象？（类似于线性图/流程图？）不进行任何的计算</span>
			<span class="c1">//只有在lines.collect()时，才进行计算，如果在此处collect()，会返回in里面的数据（string形式）</span>
     <span class="mi">2</span>      <span class="k">val</span> <span class="nv">links1</span> <span class="k">=</span> <span class="nv">lines</span><span class="o">.</span><span class="py">map</span><span class="o">{</span> <span class="n">s</span> <span class="k">=&gt;</span>
     <span class="mi">3</span>        <span class="k">val</span> <span class="nv">parts</span> <span class="k">=</span> <span class="nv">s</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">"\\s+"</span><span class="o">)</span>
     <span class="mi">4</span>        <span class="o">(</span><span class="nf">parts</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="nf">parts</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
     <span class="mi">5</span>      <span class="o">}</span>
			<span class="c1">//这里得到一个RDD对象，将lines转换成了map，但是也不会真正进行操作，只是线性图？</span>
     <span class="mi">6</span>      <span class="k">val</span> <span class="nv">links2</span> <span class="k">=</span> <span class="nv">links1</span><span class="o">.</span><span class="py">distinct</span><span class="o">()</span>
			<span class="c1">//需要分区之间的网络交流,消除重复项</span>
     <span class="mi">7</span>      <span class="k">val</span> <span class="nv">links3</span> <span class="k">=</span> <span class="nv">links2</span><span class="o">.</span><span class="py">groupByKey</span><span class="o">()</span>
			<span class="c1">//根据from URL,将link分组(可能不需要网络交流,上一步可能已经sort并把相同的key放在同一分区)</span>
			<span class="c1">//接下来迭代将会反复用到link3</span>
     <span class="mi">8</span>      <span class="k">val</span> <span class="nv">links4</span> <span class="k">=</span> <span class="nv">links3</span><span class="o">.</span><span class="py">cache</span><span class="o">()</span>
			<span class="c1">//之前的collect,会反复读取in文件,所以需要进行cache</span>
     <span class="mi">9</span>      <span class="k">var</span> <span class="n">ranks</span> <span class="k">=</span> <span class="nv">links4</span><span class="o">.</span><span class="py">mapValues</span><span class="o">(</span><span class="n">v</span> <span class="k">=&gt;</span> <span class="mf">1.0</span><span class="o">)</span>
    <span class="mi">10</span>  	<span class="c1">//把所有页面的概率初始化为1</span>
    <span class="mi">11</span>      <span class="nf">for</span> <span class="o">(</span><span class="n">i</span> <span class="k">&lt;-</span> <span class="mi">1</span> <span class="n">to</span> <span class="mi">10</span><span class="o">)</span> <span class="o">{</span>
    <span class="mi">12</span>        <span class="k">val</span> <span class="nv">jj</span> <span class="k">=</span> <span class="nv">links4</span><span class="o">.</span><span class="py">join</span><span class="o">(</span><span class="n">ranks</span><span class="o">)</span>
        	  <span class="c1">//大范围的重新组织数据,把ranks和links4的相同key item组合在一起,形成新的对象(如果spark把links4和ranks的相同				 //key放在一个分区就好了)</span>
    <span class="mi">13</span>        <span class="k">val</span> <span class="nv">contribs</span> <span class="k">=</span> <span class="nv">jj</span><span class="o">.</span><span class="py">values</span><span class="o">.</span><span class="py">flatMap</span><span class="o">{</span>
    <span class="mi">14</span>          <span class="nf">case</span> <span class="o">(</span><span class="n">urls</span><span class="o">,</span> <span class="n">rank</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="mi">15</span>            <span class="nv">urls</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">url</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">url</span><span class="o">,</span> <span class="n">rank</span> <span class="o">/</span> <span class="nv">urls</span><span class="o">.</span><span class="py">size</span><span class="o">))</span>
    <span class="mi">16</span>        <span class="o">}</span>
    <span class="mi">17</span>        <span class="n">ranks</span> <span class="k">=</span> <span class="nv">contribs</span><span class="o">.</span><span class="py">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">).</span><span class="py">mapValues</span><span class="o">(</span><span class="mf">0.15</span> <span class="o">+</span> <span class="mf">0.85</span> <span class="o">*</span> <span class="k">_</span><span class="o">)</span>
        	  <span class="c1">//通过某种算法,计算出来新的rank</span>
        	
    <span class="mi">18</span>      <span class="o">}</span>
    <span class="mi">19</span>  
    <span class="mi">20</span>      <span class="k">val</span> <span class="nv">output</span> <span class="k">=</span> <span class="nv">ranks</span><span class="o">.</span><span class="py">collect</span><span class="o">()</span>
    <span class="mi">21</span>      <span class="nv">output</span><span class="o">.</span><span class="py">foreach</span><span class="o">(</span><span class="n">tup</span> <span class="k">=&gt;</span> <span class="nf">println</span><span class="o">(</span><span class="n">s</span><span class="s">"${tup._1} has rank:  ${tup._2} ."</span><span class="o">))</span>

</code></pre></div></div>

<p>用于计算web网页的重要性。也可以表示用户访问此网站的概率。此算法不断模拟（迭代）用户的随机点击行为。最终收集的结果，会逼近网站的真正点击概率。</p>

<p>如果使用MapReduce，将会很麻烦和缓慢，因为一次模拟行为就需要一个mapreduce来模拟。（且不断的从GFS磁盘读写）</p>

<p>注意以上代码:我们并没有指定分区位置, 如何分区等操作,这些都是spark内部完成.</p>

<p>对象jj的值如下:</p>

<p><a href="https://imgchr.com/i/yhhCz8"><img src="https://s3.ax1x.com/2021/02/19/yhhCz8.png" alt="yhhCz8.png" /></a></p>

<p><strong>如何在分布式系统上执行上述计算</strong></p>

<p>1, worker在HDFS上读取数据(很可能该worker就储存了多块HDFS分片)</p>

<p>2,对读取的数据进行map  (spark支持stream操作,也就是说读数据的时候同时Map)</p>

<p>3,distinct需要worker之间的通信,具体做法是,每个worker会按照fromURL(key),把数据分成多个小份存储, worker只需要从所有服务器中,把属于自己的小份取走,形成新的分区.</p>

<p>1,2为Narrow transformation, 服务器可以本地的对数据进行操作,不需要互相通信.</p>

<p>3为wide transfromation,需要服务器之间的数据传递,非常耗时.</p>

<p>4, spark的一个优势是,在执行操作前,已经知道所有操作的lineage graph,从而有优化的空间. 所以在gruopByKey(),可以不需要在重新分区了.</p>

<h3 id="容错">容错</h3>

<p>容错是指对重新全部计算进行容错. spark的某个worker失效后,重头开始计算即可. 但存在一个问题,即该worker执行到需要wide transformation的操作时,可能需要来自其他worker的数据, spark是在内存中计算的, 此操作的结果可能被丢弃了, 为了避免其余所有的worker重新计算来产生丢弃的数据, 利用显式的persist()操作,把数据存起来.</p>

<p>比如在pagerank中，可以每10次迭代，把结果保存到HDFS。（可以用cache吗？不确定，因为cache可以只是建议性的把数据暂时保存在内存？）</p>

</article>
      </div>
    </main>

    
  </body>
</html>